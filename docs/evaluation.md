# Идейные методы оценки получившейся системы

Оценка такой сложной системы, имитирующей исследовательскую деятельность, сама по себе является нетривиальной задачей, 
требующей комплексного подхода. Необходимо оценивать не только конечный результат, но и эффективность промежуточных этапов, 
выполняемых отдельными агентами (поиск информации, извлечение данных, генерация гипотез и т.д.). Далее будут рассмотрены 
идейные подходы к оценке, основанные на анализе актуальных исследований в области оценки ИИ-ассистентов для научных исследований.


### [MLGym: A New Framework and Benchmark for Advancing AI Research Agents](https://arxiv.org/pdf/2502.14499)

В данной статье рассматриваются методы следующие методы оценки исследовательских агентов:

1.  **Использование Стандартизированных Бенчмарков и Задач:**
    *   Оценка проводится на наборе разнообразных и открытых исследовательских задач (как в MLGym-Bench), охватывающих 
        различные области ИИ (компьютерное зрение, обработка естественного языка, обучение с подкреплением, теория игр, наука о данных, 3-SAT).
    *   Каждая задача имеет свои специфические метрики производительности (например, точность, R², BLEU, перплексия, средняя награда/выигрыш, время выполнения).

2.  **Скрипты оценки и гибкие оцениваемые форматы:**
    *   Для каждой задачи предоставляется стандартизированный скрипт оценки, который автоматически вычисляет метрики.
    *   Фреймворк MLGym (который представляет статья) позволяет оценивать код стратегии в теории игр, веса модели, алгоритм RL, файл с предсказаниями.

3.  **Качественная Оценка - Уровни Возможностей (Capability Levels):**
    *   Предложена иерархическая система из 6 уровней для классификации способностей агентов:
        *   **Уровень 0:** Воспроизведение существующих исследований.
        *   **Уровень 1:** Улучшение базового решения - основной фокус MLGym-Bench.
        *   **Уровень 2:** Достижение State-of-the-Art.
        *   **Уровень 3:** Новый научный вклад.
        *   **Уровень 4:** Прорывной научный вклад.
        *   **Уровень 5:** Долгосрочная исследовательская программа.
4.  **Сравнение с Базовыми Решениями (Baselines):**
    *   Производительность агентов сравнивается с базовыми реализациями для каждой задачи, чтобы оценить, 
        насколько агент способен улучшить существующее решение (что соответствует Уровню 1).

5.  **Оценка с Учетом Затрат:**
    *   Анализируется соотношение качества и вычислительных затрат (например, стоимость API), что может быть представлено 
        в виде кривых Парето (Pareto curve) для выбора наиболее сбалансированного агента.



### [EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants](https://arxiv.org/pdf/2502.20309)
Методология EAIRA предлагает разделить методы оценки AI-моделей в качестве научных ассистентов на четыре основные категории:

1.  **Multiple Choice Questions:**
    *   **Цель:** Оценка фактических знаний и способностей к рассуждению в структурированном формате. Позволяют быстро оценить широту знаний модели.
    *   **Примеры/Разработки:** Включают как использование существующих бенчмарков (MMLU, MMLU-PRO, MATH и др.), так и разработку новых:
        *   Доменно-специфичные бенчмарки (Astronomy Benchmark, Climate Benchmark), часто генерируемые автоматически для масштабируемости.
        *   Мульти-доменный бенчмарк AI4S ("AI for Science"), сочетающий ручную и автоматическую генерацию/валидацию вопросов экспертами 
            для повышения качества и сложности, с использованием процесса AGIL для непрерывной генерации и обновления.
    *   **Ограничения:** Не могут оценить глубину рассуждений, контекстное понимание и решение проблем шаг за шагом, необходимое в научном процессе.

2.  **Open Response Benchmarks:**
    *   **Цель:** Оценка способности модели генерировать подробные, неструктурированные ответы, писать или отлаживать код, 
        демонстрируя более глубокие знания, продвинутые навыки рассуждения и решения проблем.
    *   **Примеры/Разработки:** Включают как существующие (NarrativeQA, HotpotQA, MATH, ChemistryQA и др.), так и новые:
        *   SciCode: Оценка способности генерировать научный код в различных областях, требующий понимания контекста.
        *   ALDbench: Оценка знаний в области синтеза материалов (атомно-слоевое осаждение), с ручной оценкой экспертами.
    *   **Методы оценки ответов:** Статистические метрики (BLEU, ROUGE, METEOR), семантическое сравнение (BERTScore, CheckEmbed), LLM-as-a-judge.
    *   **Ограничения:** Сложность и субъективность оценки ответов.

3.  **Лабораторные эксперименты (Lab-Style Experiments):**
    *   **Цель:** Новаторский подход для оценки моделей в контролируемой среде, имитирующей реальные исследовательские задачи (от постановки вопроса до написания отчета). 
        Позволяет детально проанализировать сильные/слабые стороны модели как ассистента в реалистичных, но контролируемых условиях, 
        включая многоэтапное планирование и реакцию на результаты. Оценка проводится *экспертами в предметной области*.
    *   **Метод:** Эксперт ставит перед моделью научную проблему и взаимодействует с ней итеративно, проходя через этапы исследования. 
        Все взаимодействия записываются и анализируются экспертами по критериям релевантности, эффективности, точности и т.д. Позволяет сравнивать разные модели или версии одной модели.
    *   **Ограничения:** Требует значительных временных затрат экспертов, не масштабируема, ограничена спецификой решаемых проблем.

4.  **Полевые эксперименты (Field-Style Experiments):**
    *   **Цель:** Новаторский подход для сбора и анализа взаимодействий между исследователями и LLM *в реальных условиях и в большом масштабе*.
        Позволяет понять потребности пользователей, выявить сильные/слабые стороны моделей и тенденции их использования в естественной среде. 
        Оценка носит *автоматизированный и масштабируемый* характер.
    *   **Метод:** Сбор и анализ тысяч реальных диалогов (промптов, ответов, поведения пользователей). 
        Оценка часто косвенная (анализ потока диалога, реакции пользователя) или автоматизированная с помощью LLM-as-a-judge по научным критериям.
    *   **Примеры:** Анализ 180 диалогов с сессии Argonne JAM с использованием Argo/O1-preview.
    *   **Ограничения:** Менее детализированная оценка, чем в лабораторных экспериментах; методы автоматического анализа (LLM-as-a-judge) требуют дальнейшей валидации.

В совокупности эти методы и аспекты формируют комплексную методологию EAIRA для оценки LLM в роли научных ассистентов, 
учитывающую как широту знаний, так и глубину рассуждений, способность к решению реальных задач, безопасность и надежность.


### [Automatic Evaluation Metrics for Artificially Generated Scientific Research](https://arxiv.org/pdf/2503.05712)

На основе исследования Automatic Evaluation Metrics for Artificially Generated Scientific Research, можно выделить следующие методы оценки качества научных работ, сгенерированных ИИ агентами:

1.  **Оценка экспертами (Domain Expert Assessments):** Это считается "золотым стандартом", но является дорогостоящим и трудоемким процессом. Часто используется для оценки лишь подмножества сгенерированных материалов.

2.  **Рецензирование с помощью больших языковых моделей (LLM-based Reviews):** Использование LLM в качестве прокси-рецензентов для автоматизации оценки. Однако исследование указывает на проблемы с надежностью этого метода, отмечая, что в некоторых случаях LLM-рецензенты показывают результаты не лучше случайного угадывания по сравнению с оценками людей, особенно для новых исследовательских гипотез.

3.  **Автоматические метрики оценки (Automatic Evaluation Metrics):** Исследование фокусируется на разработке и оценке 
      именно таких метрик как альтернативы первым двум методам. Основные предложенные автоматические метрики:
    *   **Предсказание количества цитирований (Citation Count Prediction):** Моделирование и предсказание будущего количества цитирований научной работы 
        на основе ее содержания (например, заголовка, аннотации, гипотезы, полного текста). Это используется как прокси-метрика влияния и, следовательно, качества работы. 
        Исследование показывает, что этот метод более перспективен (viable), чем предсказание оценок рецензирования. 
        Предсказание возможно даже на основе только исследовательской гипотезы, хотя и с меньшей точностью, чем по заголовку и аннотации.
    *   **Предсказание оценок рецензирования (Review Score Prediction - RSP):** Моделирование и предсказание оценок, 
        которые работа могла бы получить от рецензентов, на основе ее содержания. Этот метод также исследуется, 
        но признается более сложным, особенно при попытке обобщения между разными областями из-за различий в стандартах рецензирования и тематике. 
        Точность предсказания ниже, чем у предсказания цитирований.

### Оценивание разработанной системы

Целиком оценить итоговое исследование можно экспертной оценкой, моделями, обученными предсказывать 
количество цитирований статьи, оценить воспроизводимость существующих исследований. 
Сравнить две системы можно методом LLM-as-a-judge.

### Оценивание агентов по отдельности:

#### Table of concepts agent (генерация оглавления)
Чтобы оценить качество составления оглавления для статьи, можно брать оглавления уже готовых статей и сравнивать результат 
генерации агента с эталоном с помощью BLEU либо считать precision и recall по правильно сформулированным названиям глав.
#### Follow up questions agent (генерирование дополнительных вопросов на основе темы и названия главы по найденной информации,которые шире раскрывают тему.)

Можно на основе написанных глав генерировать список вопросов на которые там дается ответ. Затем разделить вопросы на загаданные и контекстные,
и по контекстным вопросам и тексту главы сгенерировать оберзанный текст главы без ответов на загаданные вопросы.
Таким образом у нас сформируется датасет с загаданными вопросами, темой и названием главы и контекстом.
Далее можно оценивать получилось ли у агента сгенерировать такие вопросы или нет.

#### Web page summary agent (поиск ответа на вопрос в контенте веб страницы)
Можно попросить llm сгенерировать вопросы и ответы на них по web странице, принять их за эталон, и оценивать схожесть 
ответов агента с эталоноными ответами методом LLM-as-a-judge, либо те же BLEU, BERTScore

#### Summary agent - суммаризация поисковых выдач
Можно использовать стандартные методы оценки summary по типу BERTScore

#### Questions to word agent (преобразование человеческих вопросов в набор ключевых слов для поиска статей)
Можно производить запросы в поисковую систему на некой базе запросов в виде слов, получать релевантные документы и по ним генерировать вопросы.
Таким образом получится размеченный датасет с вопросом и правильным набором слов для запроса в поисковой движок, а дальше 
оценить сходство эталонного набора слов и предсказанного по precision и recall.

#### Article relevance agent (релевантность вопроса и статьи)
Релевантность вопроса и статьи по аннотации можно оценить с помощью AUC ROC для различных порогов.

#### Chapter generation (agent генерации текста статьи по полученной информации)
Можно собрать датасет, используя написанные статьи и с помощью промпта преобразовать их в список фактов, содержашихся в них,
а затем, подавая на вход агенту список фактов, сравнивать сгенерированную статью и эталон по BLEU и иным метрикам.

#### Hypos generation agent (генерирование гипотез по переданному контексту)
Создать эталонный набор гипотез на основе заданного контекста. 
Затем оценить, насколько близки генерируемые гипотезы к эталонным.
Метрики  BLEU или BERTScore могут быть использованы для оценки сходства между сгенерированными гипотезами и эталонными гипотезами.

